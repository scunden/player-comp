{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from io import StringIO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_complete_report(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find all table elements\n",
    "        tables = soup.find_all('table')\n",
    "        \n",
    "        # Initialize a counter for naming CSV files\n",
    "        table_count = 0\n",
    "        \n",
    "        # Loop through each table\n",
    "        for table in tables:\n",
    "            # Attempt to find a caption for the table\n",
    "            caption = table.find('caption')\n",
    "            if caption is None:\n",
    "                # If no caption is found, check for the closest preceding header (h1, h2, h3, etc.)\n",
    "                for sibling in table.previous_siblings:\n",
    "                    if sibling.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n",
    "                        caption = sibling.text.strip()\n",
    "                        break\n",
    "            \n",
    "            # Use caption text or a default name if caption is not found\n",
    "            \n",
    "            if caption:\n",
    "                table_name = caption if isinstance(caption, str) else caption.text.strip()\n",
    "                table_name = table_name.replace('/', ' or ')  # Replace problematic characters in file names\n",
    "                if \"Complete\" in table_name:\n",
    "            \n",
    "                    # Convert the table to a DataFrame\n",
    "                    df = pd.read_html(str(table))[0]\n",
    "            table_count += 1\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_raw_report(df):\n",
    "    df = df[~(df.isnull().all(axis=1))]\n",
    "    df = df[~df.apply(lambda row: all(row[col] == col[1] for col in df.columns), axis=1)]\n",
    "    return df\n",
    "\n",
    "def generate_category_range(df_cat):\n",
    "    range_dict = {}\n",
    "    prev_key = None\n",
    "    for key, value in df_cat.items():\n",
    "        if prev_key is not None:\n",
    "            range_dict[(prev_key, key)] = df_cat[prev_key]\n",
    "        prev_key = key\n",
    "        \n",
    "    # Assuming the DataFrame does not exceed the maximum index from series\n",
    "    range_dict[(prev_key, float('inf'))] = df_cat[prev_key]\n",
    "    return range_dict\n",
    "\n",
    "def extract_categories(df):\n",
    "    df_cat = df[df.apply(identify_categories, axis=1)]\n",
    "    row = pd.DataFrame(data=[[\"Standard\",\"Standard\",\"Standard\"]], columns=df.columns, index=[0])\n",
    "    df_cat = pd.concat([df_cat, row]).sort_index()\n",
    "    df_cat.columns = [x[1] for x in df_cat.columns]\n",
    "    df_cat = df_cat.Statistic\n",
    "    \n",
    "    return df_cat\n",
    "\n",
    "\n",
    "def identify_categories(row):\n",
    "    return all(x == row.iloc[0] for x in row)\n",
    "\n",
    "def map_category(index, cat_range):\n",
    "    for (start, end), category in cat_range.items():\n",
    "        if start <= index < end:\n",
    "            return category\n",
    "    return None  # in case no category matches\n",
    "\n",
    "def parse_categories(df):\n",
    "    df_cat = extract_categories(df)\n",
    "    cat_range = generate_category_range(df_cat)\n",
    "\n",
    "    # Apply the function to the index of df to create a new column\n",
    "    df['Category'] = df.index.map(lambda x: map_category(x, cat_range))\n",
    "    df = df.drop(index=df_cat.index)\n",
    "    df.columns = [x[1] if x[1]!=\"\" else x[0] for x in df.columns]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve the webpage. Status code: 429\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'df' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://fbref.com/en/players/7aa8adfe/scout/365_m1/Alejandro-Garnacho-Scouting-Report\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Call the function with the specified URL\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m df_raw \u001b[38;5;241m=\u001b[39m \u001b[43mget_complete_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# df = clean_raw_report(df_raw)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# df = parse_categories(df)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 39\u001b[0m, in \u001b[0;36mget_complete_report\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to retrieve the webpage. Status code: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdf\u001b[49m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'df' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "# URL of the page to scrape\n",
    "url = 'https://fbref.com/en/players/7aa8adfe/scout/365_m1/Alejandro-Garnacho-Scouting-Report'\n",
    "\n",
    "# Call the function with the specified URL\n",
    "df_raw = get_complete_report(url)\n",
    "# df = clean_raw_report(df_raw)\n",
    "# df = parse_categories(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_players(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find the div that contains the player names\n",
    "        content_div = soup.find('div', class_='section_content')\n",
    "        \n",
    "        # Find all <a> tags within <p> tags in the div\n",
    "        a_tags = content_div.find_all('a')\n",
    "        \n",
    "        # Extract the text and href from each <a> tag\n",
    "        player_data = {a.text.strip(): a['href'] for a in a_tags}\n",
    "        \n",
    "        return player_data\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "        return {}\n",
    "\n",
    "# # URL of the page to scrape\n",
    "# url = 'https://fbref.com/en/players/az/'\n",
    "\n",
    "# # Call the function and print the results\n",
    "# player_data = get_player_link(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_all_players(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find the ul with class 'page_index'\n",
    "        ul = soup.find('ul', class_='page_index')\n",
    "        \n",
    "        # Find all <a> tags within <li> tags inside the ul\n",
    "        links = ul.find_all('a')  # Assuming each li directly contains an a\n",
    "        \n",
    "        # Extract the href and text from each <a> tag and store them in a dictionary\n",
    "        # link_ls = [retrieve_players(url+link['href']) for link in tqdm(links) if link.text.strip()]\n",
    "        link_ls=[]\n",
    "        for link in tqdm(links):\n",
    "            if link.text.strip():\n",
    "                time.sleep()\n",
    "                link_ls.extend(retrieve_players(url+link['href']))\n",
    "        \n",
    "        return link_ls\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "# URL of the page to scrape\n",
    "url = 'https://fbref.com/en/players/'\n",
    "\n",
    "# Call the function and print the results\n",
    "link_ls = get_all_players(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "player-comp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
